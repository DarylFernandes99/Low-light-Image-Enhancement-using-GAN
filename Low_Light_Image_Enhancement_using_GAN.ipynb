{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Low-Light Image Enhancement using GAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgn0Oca-prAe"
      },
      "source": [
        "# Fetching Dataset from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AlJyDJupuQX"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import shutil\n",
        "shutil.copy(\"/content/drive/<enter drive path>\", \"<enter path to save file on colab>\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyCiAtbNoHxQ"
      },
      "source": [
        "#Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaxJFRPsnW04"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.random import randint\n",
        "from tensorflow.keras import Input\n",
        "from numpy import load, zeros, ones\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras.layers import Conv2D, UpSampling2D, LeakyReLU, Activation\n",
        "from tensorflow.keras.layers import Concatenate, Dropout, BatchNormalization, LeakyReLU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKEC_Mfopb2s"
      },
      "source": [
        "# Creating Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTesJLExoK9h"
      },
      "source": [
        "## Defining Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbxlfZtrnc7c"
      },
      "source": [
        "def define_discriminator(image_shape):\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        " \n",
        "\tin_src_image = Input(shape=image_shape)\n",
        "\tin_target_image = Input(shape=image_shape)\n",
        " \n",
        "\tmerged = Concatenate()([in_src_image, in_target_image])\n",
        " \n",
        "\td = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        " \n",
        "\td = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
        "\td = BatchNormalization()(d)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        " \n",
        "\td = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
        "\td = BatchNormalization()(d)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        " \n",
        "\td = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
        "\td = BatchNormalization()(d)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        " \n",
        "\td = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
        "\td = BatchNormalization()(d)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        " \n",
        "\td = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
        "\tpatch_out = Activation('sigmoid')(d)\n",
        " \n",
        "\tmodel = Model([in_src_image, in_target_image], patch_out)\n",
        " \n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWqF429qoP1H"
      },
      "source": [
        "## Defining Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71cxSSWUnhIq"
      },
      "source": [
        "def define_generator(image_shape = (256, 256, 3)):\n",
        "    init = RandomNormal(stddev=0.02)\n",
        "    in_image = Input(shape=image_shape)\n",
        "\n",
        "    g = Conv2D(64, (7,7), padding='same', kernel_initializer=init)(in_image)\n",
        "    g = BatchNormalization()(g, training=True)\n",
        "    g3 = LeakyReLU(alpha=0.2)(g)\n",
        "\n",
        "    g = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g3)\n",
        "    g = BatchNormalization()(g, training=True)\n",
        "    g2 = LeakyReLU(alpha=0.2)(g)\n",
        "\n",
        "    g = Conv2D(256, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g2)\n",
        "    g = BatchNormalization()(g, training=True)\n",
        "    g1 = LeakyReLU(alpha=0.2)(g)\n",
        "\n",
        "    for _ in range(6):\n",
        "        g = Conv2D(256, (3,3), padding='same', kernel_initializer=init)(g1)\n",
        "        g = BatchNormalization()(g, training=True)\n",
        "        g = LeakyReLU(alpha=0.2)(g)\n",
        "\n",
        "        g = Conv2D(256, (3,3), padding='same', kernel_initializer=init)(g)\n",
        "        g = BatchNormalization()(g, training=True)\n",
        "\n",
        "        g1 = Concatenate()([g, g1])\n",
        "\n",
        "    g = UpSampling2D((2, 2))(g1)\n",
        "    g = Conv2D(128, (1, 1), kernel_initializer=init)(g)\n",
        "    g = Dropout(0.5)(g, training=True)\n",
        "    g = Concatenate()([g, g2])\n",
        "    g = BatchNormalization()(g, training=True)\n",
        "    g = LeakyReLU(alpha=0.2)(g)\n",
        "\n",
        "    g = UpSampling2D((2, 2))(g)\n",
        "    g = Conv2D(64, (1, 1), kernel_initializer=init)(g)\n",
        "    g = Dropout(0.5)(g, training=True)\n",
        "    g = Concatenate()([g, g3])\n",
        "    g = BatchNormalization()(g, training=True)\n",
        "    g = LeakyReLU(alpha=0.2)(g)\n",
        "\n",
        "    g = Conv2D(3, (7,7), padding='same', kernel_initializer=init)(g)\n",
        "    g = BatchNormalization()(g, training=True)\n",
        "    out_image = Activation('tanh')(g)\n",
        "\n",
        "    model = Model(in_image, out_image)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj8HuE2qoS3f"
      },
      "source": [
        "## Initializing GAN training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTqa4cLEnkmV"
      },
      "source": [
        "def define_gan(g_model, d_model, image_shape):\n",
        "\t# make weights in the discriminator not trainable\n",
        "\tfor layer in d_model.layers:\n",
        "\t\tif not isinstance(layer, BatchNormalization):\n",
        "\t\t\tlayer.trainable = False\n",
        "\t# define the source image\n",
        "\tin_src = Input(shape=image_shape)\n",
        "\t# connect the source image to the generator input\n",
        "\tgen_out = g_model(in_src)\n",
        "\t# connect the source input and generator output to the discriminator input\n",
        "\tdis_out = d_model([in_src, gen_out])\n",
        "\t# src image as input, generated image and classification output\n",
        "\tmodel = Model(in_src, [dis_out, gen_out])\n",
        "\t# compile model\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss=['binary_crossentropy', 'mae'], optimizer=opt, loss_weights=[1,100])\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBFQpdz5oX4s"
      },
      "source": [
        "## Loading Real Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQpWSsK0nolM"
      },
      "source": [
        "def load_real_samples(filename):\n",
        "\t# load compressed arrays\n",
        "\tdata = load(filename)\n",
        "\t# unpack arrays\n",
        "\tX1, X2 = data['arr_0'], data['arr_1']\n",
        "\t# scale from [0,255] to [-1,1]\n",
        "\tX1 = (X1 - 127.5) / 127.5\n",
        "\tX2 = (X2 - 127.5) / 127.5\n",
        "\treturn [X2, X1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZQ9GO20oZgQ"
      },
      "source": [
        "## Generating Real Fake Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlngHBPLnrof"
      },
      "source": [
        "def generate_real_samples(dataset, n_samples, patch_shape):\n",
        "\t# unpack dataset\n",
        "\ttrainA, trainB = dataset\n",
        "\t# choose random instances\n",
        "\tix = randint(0, trainA.shape[0], n_samples)\n",
        "\t# retrieve selected images\n",
        "\tX1, X2 = trainA[ix], trainB[ix]\n",
        "\t# generate 'real' class labels (1)\n",
        "\ty = ones((n_samples, patch_shape, patch_shape, 1))\n",
        "\treturn [X1, X2], y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0nsxrucoepT"
      },
      "source": [
        "## Generating Fake Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAnQEQxNnuJD"
      },
      "source": [
        "def generate_fake_samples(g_model, samples, patch_shape):\n",
        "\t# generate fake instance\n",
        "\tX = g_model.predict(samples)\n",
        "\t# create 'fake' class labels (0)\n",
        "\ty = zeros((len(X), patch_shape, patch_shape, 1))\n",
        "\treturn X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qs-bEhiohte"
      },
      "source": [
        "## Summarizing Training and Saving Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmzXpDOanw3z"
      },
      "source": [
        "def summarize_performance(step, g_model, d_model, dataset, n_samples=3):\n",
        "    # select a sample of input images\n",
        "    [X_realA, X_realB], _ = generate_real_samples(dataset, n_samples, 1)\n",
        "    # generate a batch of fake samples\n",
        "    X_fakeB, _ = generate_fake_samples(g_model, X_realA, 1)\n",
        "    # scale all pixels from [-1,1] to [0,1]\n",
        "    X_realA = (X_realA + 1) / 2.0\n",
        "    X_realB = (X_realB + 1) / 2.0\n",
        "    X_fakeB = (X_fakeB + 1) / 2.0\n",
        "    # plot real source images\n",
        "    plt.figure(figsize=(14, 14))\n",
        "    for i in range(n_samples):\n",
        "        plt.subplot(3, n_samples, 1 + i)\n",
        "        plt.axis('off')\n",
        "        plt.title('Low-Light')\n",
        "        plt.imshow(X_realA[i])\n",
        "    # plot generated target image\n",
        "    for i in range(n_samples):\n",
        "        plt.subplot(3, n_samples, 1 + n_samples + i)\n",
        "        plt.axis('off')\n",
        "        plt.title('Generated')\n",
        "        plt.imshow(X_fakeB[i])\n",
        "    # plot real target image\n",
        "    for i in range(n_samples):\n",
        "        plt.subplot(3, n_samples, 1 + n_samples*2 + i)\n",
        "        plt.axis('off')\n",
        "        plt.title('Ground Truth')\n",
        "        plt.imshow(X_realB[i])\n",
        "    # save plot to file\n",
        "    filename1 = step_output + 'plot_%06d.png' % (step+1)\n",
        "    plt.savefig(filename1)\n",
        "    plt.close()\n",
        "    # save the generator model\n",
        "    filename2 = model_output + 'gen_model_%06d.h5' % (step+1)\n",
        "    g_model.save(filename2)\n",
        "    # save the discriminator model\n",
        "    filename3 = model_output + 'disc_model_%06d.h5' % (step+1)\n",
        "    d_model.save(filename3)\n",
        "    print('[.] Saved Step : %s' % (filename1))\n",
        "    print('[.] Saved Model: %s' % (filename2))\n",
        "    print('[.] Saved Model: %s' % (filename3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIChDKI8omnh"
      },
      "source": [
        "## Training Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INv0tUj3n0cj"
      },
      "source": [
        "def train(d_model, g_model, gan_model, dataset, n_epochs=100, n_batch=12):\n",
        "    # determine the output square shape of the discriminator\n",
        "    n_patch = d_model.output_shape[1]\n",
        "    # unpack dataset\n",
        "    trainA, trainB = dataset\n",
        "    # calculate the number of batches per training epoch\n",
        "    bat_per_epo = int(len(trainA) / n_batch)\n",
        "    # calculate the number of training iterations\n",
        "    n_steps = bat_per_epo * n_epochs\n",
        "    print(\"[!] Number of steps {}\".format(n_steps))\n",
        "    print(\"[!] Saves model/step output at every {}\".format(bat_per_epo * 1))\n",
        "    # manually enumerate epochs\n",
        "    for i in range(n_steps):\n",
        "        start = time.time()\n",
        "        # select a batch of real samples\n",
        "        [X_realA, X_realB], y_real = generate_real_samples(dataset, n_batch, n_patch)\n",
        "        # generate a batch of fake samples\n",
        "        X_fakeB, y_fake = generate_fake_samples(g_model, X_realA, n_patch)\n",
        "        # update discriminator for real samples\n",
        "        d_loss1 = d_model.train_on_batch([X_realA, X_realB], y_real)\n",
        "        # update discriminator for generated samples\n",
        "        d_loss2 = d_model.train_on_batch([X_realA, X_fakeB], y_fake)\n",
        "        # update the generator\n",
        "        g_loss, _, _ = gan_model.train_on_batch(X_realA, [y_real, X_realB])\n",
        "        # summarize performance\n",
        "        time_taken = time.time() - start\n",
        "        print(\n",
        "            '[*] %06d, d1[%.3f] d2[%.3f] g[%06.3f] ---> time[%.2f], time_left[%.08s]'\n",
        "                %\n",
        "            (i+1, d_loss1, d_loss2, g_loss, time_taken, str(datetime.timedelta(seconds=((time_taken) * (n_steps - (i + 1))))).split('.')[0].zfill(8))\n",
        "        )\n",
        "        # summarize model performance\n",
        "        if (i+1) % (bat_per_epo * 1) == 0:\n",
        "            summarize_performance(i, g_model, d_model, dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ir5XcIdo8XF"
      },
      "source": [
        "# Main Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk1JU6YXo1e7"
      },
      "source": [
        "## Loading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jX2Qli9o1CW"
      },
      "source": [
        "dataset = load_real_samples('<enter path to save file on colab>')\n",
        "print('Loaded', dataset[0].shape, dataset[1].shape)\n",
        "image_shape = dataset[0].shape[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qst6rVopRJT"
      },
      "source": [
        "## Defining Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "827JDijxpQm6"
      },
      "source": [
        "d_model = define_discriminator(image_shape)\n",
        "g_model = define_generator(image_shape)\n",
        "gan_model = define_gan(g_model, d_model, image_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WcK8E0fo_Ae"
      },
      "source": [
        "## Creating model Directory and Calling Train Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPuLnNjrn6kv"
      },
      "source": [
        "dir = '<enter path to save model on colab>'\n",
        "fileName = 'Enhancement Model'\n",
        "step_output = dir + fileName + \"/Step Output/\"\n",
        "model_output = dir + fileName + \"/Model Output/\"\n",
        "if fileName not in os.listdir(dir):\n",
        "    os.mkdir(dir + fileName)\n",
        "    os.mkdir(step_output)\n",
        "    os.mkdir(model_output)\n",
        "\n",
        "train(d_model, g_model, gan_model, dataset, batch=12)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}